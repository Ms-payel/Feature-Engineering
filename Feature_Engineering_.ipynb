{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "   \n",
        "   Parameters are internal configuration variables in a machine learning model that are learned from the data during the training process and are used to make predictions. They are distinct from hyperparameters, which are external settings that are manually set before training begins and control the learning process itself. Examples of parameters include the weights and biases in a neural network and the coefficients in a linear regression model.\n",
        "\n",
        "   In machine learning, model parameters mainly come in 2 types: weights and biases. In the example of a simple linear regression model, y = m x + b , the weight corresponds to the slope m, controlling how strongly the input influences the output. The larger the weight, the more impact of the input."
      ],
      "metadata": {
        "id": "M1s8iFWOqpNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is correlation, What does negative correlation mean?\n",
        "\n",
        "  Correlation is a statistical measure showing how two variables move together, while negative correlation (or inverse correlation) means they move in opposite directions: as one variable increases, the other tends to decrease, and vice-versa, like temperature and winter coat sales. The strength of this relationship is measured by the correlation coefficient (r), which ranges from -1 (perfect negative) to +1 (perfect positive), with 0 meaning no linear link.\n",
        "\n",
        "  Negative Correlation:\n",
        " *  Definition: An inverse relationship where two variables move in opposite directions.\n",
        "* Example: As study time increases, exam scores might decrease (in some cases), or as the price of a product goes down, the demand for it goes up.\n",
        "* Coefficient: A negative correlation coefficient (r) is a value between -1 and 0. A value near -1 signifies a strong negative relationship, while a value near 0 indicates a weak one.\n",
        "  \n"
      ],
      "metadata": {
        "id": "XclmAqUbsFW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "   \n",
        "   Machine Learning (ML) enables systems to learn patterns from data to make predictions or decisions without explicit programming, using algorithms to build predictive models from data, which are then trained and evaluated, requiring core components like Data, Algorithms, Models, Training, and Evaluation/Optimization.\n",
        "\n",
        "   Main Components in Machine Learning:\n",
        "\n",
        "* Data: The raw material (historical information) used to teach the system, containing features and labels from which patterns are extracted.\n",
        "* Algorithms: The set of rules or procedures (e.g., linear regression, neural networks) that learn from the data to find relationships.\n",
        "* Models: The output or result of an algorithm after it has been trained on data; it's the mathematical representation that makes predictions.\n",
        "* Training: The process of feeding data to the algorithm to adjust its internal parameters, allowing it to learn patterns and build the model.\n",
        "* Evaluation/Metrics: Methods (like accuracy, recall) to assess how well the trained model performs on unseen data and identify areas for improvement.\n",
        "* Optimization: Techniques (like gradient descent) used during training to minimize errors and improve the model's performance.\n",
        "* Features: Specific, measurable properties or attributes in the data that the model uses for learning and prediction, often involving extraction and selection."
      ],
      "metadata": {
        "id": "1bxJpJ9ru0Ef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "   A loss value is a fundamental metric that quantifies the difference between a model's predicted output and the actual target values. It is a critical indicator in determining a model's quality, though it should be interpreted alongside other metrics [1].\n",
        "\n",
        "Here is how loss value helps:\n",
        "\n",
        "Quantifying Model Error-\n",
        "The primary function of the loss value (or \"cost\" function) is to provide a single numerical measure of how well (or poorly) the model is performing its task [1].\n",
        "* Lower Loss = Better Fit: A lower loss value indicates that the model's predictions are closer to the true values. This suggests the model has learned the patterns in the training data effectively.\n",
        "* Higher Loss = Worse Fit: A higher loss value means the model's predictions frequently deviate significantly from the true values, indicating poor performance [1].\n",
        "\n",
        "Guiding the Learning Process-\n",
        "During the training phase, machine learning algorithms like gradient descent use the loss value to adjust the model's internal parameters (weights and biases). The goal of training is always to minimize this loss value [1, 2]. By repeatedly calculating the loss and making small adjustments, the model iteratively improves its accuracy.\n",
        "\n",
        "Diagnosing Model Issues (Training vs. Validation Loss)-A single loss value in isolation is useful, but comparing the loss across different datasets (training vs. validation) provides deeper insights into specific model issues:\n",
        "\n",
        "Comparison and Selection-Loss values allow for objective comparison between different models or different configurations (hyperparameters) of the same model. When choosing between two candidate models designed for the same task, the one that consistently achieves a lower validation loss is generally considered the better performer [1]."
      ],
      "metadata": {
        "id": "6cCSsxhZv4t_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        "   In machine learning, Categorical Variables represent distinct groups (like colors, genders) with no inherent order, while Continuous Variables (or Numerical) can take any value within a range (like height, temperature) and are key to distinguishing between classification (categorical output) and regression (continuous output) tasks, with data preprocessing often needed for categorical data.\n",
        "\n",
        "   **Categorical Variables**\n",
        "* Definition: Non-numeric data representing qualities or labels that fall into finite categories.\n",
        "* Types:\n",
        "Nominal: Categories with no order (e.g., Gender: Male/Female, Blood Group: A/B/O/AB).\n",
        "\n",
        "     Ordinal: Categories with a meaningful order (e.g., Education Level: High School/Bachelor's/Master's, Sizes: Small/Medium/Large).\n",
        "\n",
        "  ML Use: Crucial for classification problems (predicting a category like \"spam\" or \"not spam\").\n",
        "\n",
        "  **Continuous Variables (Numerical)**\n",
        "* Definition: Data that can be measured and can take any value within a given range, often involving decimals.\n",
        "* Types:\n",
        "Discrete: Countable whole numbers (e.g., number of children, items).\n",
        "\n",
        "  Continuous: Measured values (e.g., height, weight, age, temperature, stock prices).\n",
        "\n",
        "  ML Use: Key for regression problems (predicting a specific numerical value like house price).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O0Q6r6VIAWpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "  \n",
        "  Categorical variables represent groupings or qualities rather than numerical quantities (e.g., colors, types of cities, yes/no responses). Most machine learning algorithms require numerical input, so these variables must be converted.\n",
        "The handling of categorical variables is often split into two main types: nominal (no inherent order, like \"color\") and ordinal (has a rank/order, like \"education level\").\n",
        "Here are the common techniques for encoding categorical variables:\n",
        "\n",
        "Detailed Breakdown of Techniques\n",
        "1. One-Hot Encoding (OHE)-\n",
        "This is perhaps the most widely used technique, especially for nominal data where assuming an order would be incorrect.\n",
        "\n",
        "* How it works: If you have a variable Color with values Red, Green, and Blue, OHE creates three new columns: Color_Red, Color_Green, and Color_Blue. A row with \"Red\" will have a 1 in Color_Red and 0 in the other two.\n",
        "\n",
        "* Pros: Does not assume ordinality; prevents the model from interpreting categories as being closer together than they are.\n",
        "\n",
        "* Cons: Can create a very large number of features (high dimensionality) if a variable has many unique categories (high cardinality).\n",
        "\n",
        "2. Label Encoding / Ordinal Encoding-\n",
        "These methods are used when the order of the categories matters.\n",
        "\n",
        "* How it works: You map categories to sequential integers. For example, a Size variable might become: \"Small\" = 0, \"Medium\" = 1, \"Large\" = 2.\n",
        "* Pros: Reduces dimensionality; very simple to implement.\n",
        "* Cons: If applied to nominal data, the model might incorrectly infer that \"Red\" (1) is somehow \"less than\" or \"smaller than\" \"Blue\" (2).\n",
        "\n",
        "3. Target Encoding (Mean Encoding)\n",
        "This technique is useful when you have a categorical variable with many levels and a target variable (e.g., predicting house prices based on ZipCode).\n",
        "* How it works: Each category value is replaced by the average of the target variable for that specific category. For example, you replace every occurrence of \"ZipCode 90210\" with the average house price in that zip code.\n",
        "* Pros: Good for high-cardinality nominal variables; captures information about the target variable efficiently.\n",
        "* Cons: Can be prone to data leakage if not cross-validated properly; sensitive to noise and outliers.\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "S7VSy-7VCapF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "   \n",
        "   Training a Dataset:-\n",
        "Training is the process of using a large portion of your data (the \"training set\") to teach a machine learning algorithm how to recognize patterns, relationships, and features.\n",
        "* Purpose: The algorithm learns from this input to make accurate predictions or decisions in the future. It adjusts its internal parameters (or \"weights\") repeatedly until it can map input data to the desired output with an acceptable level of accuracy.\n",
        "* Analogy: If you want to teach a child to identify different animals, you show them many pictures of cats, dogs, birds, etc., telling them which one is which. The child learns the distinguishing features of each animal through repeated exposure. The pictures are your training set.\n",
        "\n",
        "Testing a Dataset:-\n",
        "Testing is the process of evaluating the performance of the already-trained model using a separate, independent portion of the data (the \"testing set\") that the model has never seen before.\n",
        "* Purpose: The testing set acts as a real-world assessment. Because the model hasn't memorized these specific data points during training, its performance on the testing set is a reliable indicator of how well it can generalize its learning to new, unseen data—which is the ultimate goal.\n",
        "* Analogy: To test the child's knowledge, you show them new pictures of animals they haven't seen before and ask them to identify them. Their success rate on these new pictures shows how well they genuinely learned the concept, rather than just memorizing the pictures you showed them initially."
      ],
      "metadata": {
        "id": "aQLBeXgRFl0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "   The sklearn.preprocessing module in the scikit-learn library provides essential utility functions and transformer classes to prepare raw data for use in machine learning models. This process is crucial because many algorithms perform better or expect data to be in a specific format, such as centered around zero, scaled to a consistent range, or numerically encoded.\n",
        "\n",
        "   Key Functions and Transformers\n",
        "The module offers a variety of tools for common data preprocessing tasks:\n",
        "* Feature Scaling: This is vital for algorithms sensitive to the magnitude of feature values (e.g., K-Nearest Neighbors, Support Vector Machines, linear models).\n",
        "* StandardScaler: Standardizes features by removing the mean and scaling to unit variance (Z-score normalization), resulting in a distribution with a mean of 0 and a standard deviation of 1.\n",
        "* MinMaxScaler: Rescales features to a specified range, typically between 0 and 1, which is useful for neural networks or algorithms that require bounded inputs.\n",
        "* RobustScaler: Uses statistics robust to outliers (median and interquartile range) to scale features, making it suitable for data containing many outliers.\n",
        "* MaxAbsScaler: Scales each feature by its maximum absolute value, ensuring values lie within the range [-1, 1]. It is designed for sparse data where preserving zero entries is important.\n",
        "\n",
        "Categorical Data Encoding: Machine learning models generally require numerical input, so categorical features must be converted.\n",
        "\n",
        "* OneHotEncoder: Transforms categorical integer features into a one-hot encoded matrix. Each category is converted into a new binary column, preventing models from assuming an arbitrary order between categories.\n",
        "* LabelEncoder: Encodes target labels with values between 0 and \\(n\\_classes-1\\). This is used for target variables (y), not typically for input features (X), as it can introduce an incorrect assumption of order.\n",
        "\n",
        "Missing Value Imputation: Real-world data often has missing values, which are incompatible with scikit-learn estimators.\n",
        "* SimpleImputer: Replaces missing values (often encoded as NaN) using a statistical measure such as the mean, median, or most frequent value of the column or row.\n",
        "\n",
        "Other Transformations:\n",
        "* Binarizer: Applies a threshold to numerical features to get boolean values (0 or 1).\n",
        "* PolynomialFeatures: Generates polynomial and interaction features to add complexity and non-linearity to a model.\n",
        "* FunctionTransformer: Allows the application of an arbitrary Python function as a transformer within a scikit-learn pipeline.\n",
        "\n",
        "Integration with Pipelines\n",
        "These preprocessing classes implement the scikit-learn Transformer API (fit, transform, and fit_transform methods), making them highly suitable for use within a sklearn.pipeline.Pipeline or sklearn.compose.ColumnTransformer. This enables the chaining of multiple preprocessing steps and model training into a single, cohesive workflow, ensuring that the same transformations learned from the training data are applied consistently to new test data.\n",
        "\n",
        "\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "_w44DF3rHPQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "\n",
        "   ‍The test set is a portion (or partition) of the available training data that is “held back” and not used during model training. The purpose of the test set is to evaluate the performance of the model on unseen data after it has been trained. As such, the test set should not be explicitly or implicitly used during training or hyperparameter tuning. The test set is typically 10-30% of the training data.\n",
        "\n",
        "   How is a test set defined:-\n",
        "   \n",
        "   A test set is defined by splitting the original dataset into distinct partitions. The dataset is typically divided into three parts: the training set, the validation set, and the test set. The training set is used to train the model, the validation set is used for hyperparameter tuning and model selection, and the test set is used to evaluate the final model performance.\n",
        "\n",
        "The splitting process should ensure that the distribution of data in the test set is representative of the overall dataset. This can be achieved by using random sampling, time-series partitions, or stratified sampling, depending on the nature of the data. In time-series partitions, you typically create the train/validation sets from an earlier time range of data, and the test set from a time range after the train set (to test if the model can generalize to work well in future unseen data). In stratified sampling, the data is divided in such a way that the proportion of each class or category in the test set is the same as in the original dataset, ensuring a balanced representation of different classes or categories."
      ],
      "metadata": {
        "id": "-yz21QgoeSPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "  One of the most important steps in preparing data for training a ML model is splitting the dataset into training and testing sets. This simply means dividing the data into two parts: one to train the machine learning model (training set), and another to evaluate how well it performs on unseen data (testing set). The training set is used to fit the model, and the statistics of the training set are known. The second set is called the test data set which is solely used for predictions.\n",
        "\n",
        "  The scikit-learn library can be installed using pip:-\n",
        "  pip install scikit-learn\n",
        "\n",
        "  Dataset Splitting:-\n",
        "Scikit-learn is one of the most widely used machine learning libraries in Python. It provides a range of tools for building models, pre-processing data, and evaluating performance. For splitting datasets, it provides a handy function called train_test_split() within the model_selection module, making it simple to divide your data into training and testing sets.\n",
        "\n",
        "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
        "\n",
        "Parameters:\n",
        "\n",
        "* arrays: The data you want to split. This can be in the form of lists, arrays, pandas DataFrames, or matrices.\n",
        "* test_size: A number between 0.0 and 1.0 that tells what portion of the data should go into the test set. For example, 0.2 means 20% of the data will be used for testing.\n",
        "* train_size: this is a number between 0.0 and 1.0 that tells what portion of the data should go into the training set. If not set, it’s automatically calculated based on the test_size.\n",
        "random_state: A number that makes sure the split is the same every time you run the code. It’s like setting a seed for the shuffle.\n",
        "* shuffle: If True, the data is shuffled before splitting. This helps make the train and test sets more random. It’s True by default.\n",
        "* stratify: This helps keep the same class distribution in both the train and test sets. It’s useful especially for classification problems.\n",
        "\n",
        "In the example, we first import pandas and sklearn. Then, we load the CSV file using the read_csv() function. This stores the data in a DataFrame called df. we want to predict the house price, which is in the last column so we set that as y (target). All the other columns are used as features, stored in X.\n",
        "\n",
        "We use train_test_split() to split the data:\n",
        "\n",
        "test_size=0.05 means 5% of the data is used for testing, and 95% for training.\n",
        "random_state=0 ensures the split is the same every time we run the code.\n",
        "\n"
      ],
      "metadata": {
        "id": "WeJLz3ltfHMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv('Real-estate.csv')\n",
        "\n",
        "df.head()\n",
        "\n",
        "# read the dataset\n",
        "df = pd.read_csv('Real-estate.csv')\n",
        "\n",
        "# verify\n",
        "print(df.head())\n",
        "\n",
        "# get the locations\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "# split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.05, random_state=0\n",
        ")\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"Testing data shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "vdWA8UTHiUVs",
        "outputId": "c9e9ff0a-947d-448b-bf9e-cbccadc69795"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-019d4eb5-c574-45ae-9f75-970df3d2aac6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-019d4eb5-c574-45ae-9f75-970df3d2aac6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Real-estate.csv to Real-estate (8).csv\n",
            "   No  X1 transaction date  X2 house age  \\\n",
            "0   1             2012.917          32.0   \n",
            "1   2             2012.917          19.5   \n",
            "2   3             2013.583          13.3   \n",
            "3   4             2013.500          13.3   \n",
            "4   5             2012.833           5.0   \n",
            "\n",
            "   X3 distance to the nearest MRT station  X4 number of convenience stores  \\\n",
            "0                                84.87882                               10   \n",
            "1                               306.59470                                9   \n",
            "2                               561.98450                                5   \n",
            "3                               561.98450                                5   \n",
            "4                               390.56840                                5   \n",
            "\n",
            "   X5 latitude  X6 longitude  Y house price of unit area  \n",
            "0     24.98298     121.54024                        37.9  \n",
            "1     24.98034     121.53951                        42.2  \n",
            "2     24.98746     121.54391                        47.3  \n",
            "3     24.98746     121.54391                        54.8  \n",
            "4     24.97937     121.54245                        43.1  \n",
            "Training data shape: (393, 7)\n",
            "Testing data shape: (21, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "    Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore data, and possibly formulate hypotheses that might cause new data collection and experiments. EDA focuses more narrowly on checking assumptions required for model fitting and hypothesis testing. It also checks while handling missing values and making transformations of variables as needed.\n",
        "\n",
        "EDA builds a robust understanding of the data, and issues associated with either the info or process. It's a scientific approach to getting the story of the data.\n",
        "\n",
        "Key reasons for performing EDA first:\n",
        "* Data Quality Assessment: Identifies errors, missing data (nulls), inconsistencies, and outliers that can severely corrupt model training.\n",
        "* Understand Data Characteristics: Reveals distributions, shapes, scales, and unique traits of individual variables (e.g., skewed, normal).\n",
        "* Detect Relationships & Patterns: Uncovers correlations, trends, and subgroups between variables, guiding feature selection and engineering.\n",
        "* Inform Model Selection: Helps choose appropriate algorithms; for instance, visualizing a skewed target variable suggests non-linear models might be better.\n",
        "* Feature Engineering: Provides insights for creating new, more informative features from existing ones.\n",
        "* Prevent Data Leakage: Performing EDA on the entire dataset before splitting helps you understand the full picture without contaminating test data, ensuring realistic performance evaluation.\n",
        "Guide Preprocessing: Determines if you need scaling, transformation, or imputation strategies, all crucial for model success."
      ],
      "metadata": {
        "id": "gvEJQ0oD1wTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "    \n",
        "    Correlation is a statistical measure showing how two variables move together, indicating the strength and direction of their linear relationship, from perfect positive (+1, same direction) to perfect negative (-1, opposite directions), with 0 meaning no relationship; it helps spot trends but doesn't prove causation.\n",
        "\n",
        "    Key aspects of correlation:\n",
        "* Measures Relationship: It quantifies how changes in one variable are associated with changes in another.\n",
        "* Correlation Coefficient (r): A value between -1 and +1, where:\n",
        "\n",
        "+1: Perfect positive correlation (both variables increase/decrease together).\n",
        "\n",
        "-1: Perfect negative correlation (one increases as the other decreases).\n",
        "\n",
        "0: No correlation (no linear relationship).\n",
        "\n",
        "Strength: The closer the value is to +1 or -1, the stronger the relationship; closer to 0 means weaker.\n",
        "\n",
        "Types:\n",
        "* Positive: Variables move in the same direction (e.g., study hours and test scores).\n",
        "* Negative: Variables move in opposite directions (e.g., temperature and heating costs).\n",
        "* Zero/No: No discernible pattern.\n",
        "\n",
        "Correlation vs. Causation: A critical point is that correlation does not mean one variable causes the other; it only shows they are related.\n",
        "\n",
        "Example:\n",
        "If ice cream sales and temperature have a strong positive correlation, it means as temperature rises, sales rise, but the heat doesn't make people buy ice cream (though it encourages it); other factors are involved."
      ],
      "metadata": {
        "id": "fiPp271B3d7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "\n",
        "    Negative correlation (or inverse correlation) means two variables move in opposite directions: as one increases, the other tends to decrease, and vice versa, like temperature and sales of woollen clothes. It's measured by the correlation coefficient (r), which ranges from -1 (perfect negative) to +1 (perfect positive), with values closer to -1 indicating a stronger inverse relationship.  \n",
        "\n",
        "    Key Characteristics\n",
        "* Opposite Movement: When one variable goes up, the other goes down.\n",
        "* Inverse Relationship: Also called an inverse correlation.\n",
        "* Correlation Coefficient (r): A value between -1 and 1; a negative number indicates negative correlation (e.g., -0.8).\n",
        "* Downward Slope: On a graph, it appears as a line sloping downwards from left to right.\n",
        "\n",
        "Examples\n",
        "* Price & Demand: As the price of a product increases, the quantity demanded often decreases.\n",
        "* Temperature & Clothing Sales: As outside temperature rises, sales of heavy winter coats decrease.\n",
        "* Sleep & Mistakes: More hours of sleep are generally associated with fewer mistakes made.\n"
      ],
      "metadata": {
        "id": "d6oiP0j04lsY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "    In Python, you can find the correlation between variables using libraries such as Pandas, NumPy, and SciPy, with Pandas being the most common method for data analysis. Visualizations like heatmaps and scatter plots help interpret the results.\n",
        "\n",
        "    Using Pandas (Recommended for DataFrames)\n",
        "The corr() method on a Pandas DataFrame calculates the pairwise correlation coefficient for all numeric columns, returning a correlation matrix. By default, it uses the Pearson correlation method.\n",
        "\n",
        "* Calculate a correlation matrix for all numeric columns:\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "Create a sample DataFrame\n",
        "\n",
        "data = {'x': [45, 37, 42, 35, 39],\n",
        "        'y': [38, 31, 26, 28, 33],\n",
        "        'z': [10, 15, 17, 21, 12]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "Calculate the correlation matrix\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n",
        "\n",
        "The output is a table (matrix) showing the correlation of each variable with every other variable.\n",
        "\n",
        "* Calculate the correlation between two specific columns:\n",
        "\n",
        "Calculate the correlation between 'x' and 'y'\n",
        "\n",
        "correlation_xy = df['x'].corr(df['y'])\n",
        "\n",
        "print(f\"Correlation between x and y: {correlation_xy}\")\n",
        "\n",
        "* Specify the correlation method:\n",
        "You can use different methods like 'kendall' (Kendall Tau) or 'spearman' (Spearman rank correlation) by passing the method argument.\n",
        "\n",
        "spearman_corr = df.corr(method='spearman')\n",
        "\n",
        "* Using NumPy\n",
        "\n",
        "For simple arrays or when you don't need a full DataFrame, NumPy's corrcoef() function is efficient. It returns a correlation matrix between two or more arrays.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = [215, 325, 185, 332, 406]\n",
        "y = [14.2, 16.4, 11.9, 15.2, 18.5]\n",
        "\n",
        "Calculate the correlation matrix\n",
        "\n",
        "matrix = np.corrcoef(x, y)\n",
        "\n",
        "print(matrix)\n",
        "\n",
        "The off-diagonal values (e.g., matrix[0, 1] or matrix[1, 0]) are the correlation coefficients\n",
        "\n",
        "* Using SciPy\n",
        "\n",
        "The scipy.stats.pearsonr() function is useful if you need the correlation coefficient along with the p-value, which indicates the statistical significance of the correlation.\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "x = [215, 325, 185, 332, 406]\n",
        "y = [14.2, 16.4, 11.9, 15.2, 18.5]\n",
        "\n",
        "Calculate Pearson correlation coefficient and p-value\n",
        "\n",
        "corr_coefficient, p_value = stats.pearsonr(x, y)\n",
        "\n",
        "print(f\"Pearson Correlation Coefficient: {corr_coefficient}\")\n",
        "\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "* Visualizing Correlation with a Heatmap-\n",
        "Visualizing the correlation matrix using Seaborn is a great way to quickly spot patterns.\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Using the DataFrame 'df' from the Pandas example\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "* Interpreting the Results\n",
        "\n",
        "The correlation coefficient value ranges from -1 to +1.\n",
        "\n",
        "A value of +1 indicates a perfect positive linear correlation (both variables increase together).\n",
        "\n",
        "A value of -1 indicates a perfect negative linear correlation (as one increases, the other decreases).\n",
        "\n",
        "A value of 0 indicates no linear relationship.\n",
        "\n",
        "Note that these methods primarily measure linear relationships. Variables can have a strong non-linear relationship even if their Pearson correlation coefficient is close to zero.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DfCmMyz55W1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "    Causation is when one event directly causes another, while correlation is a relationship where two events happen together but don't necessarily cause each other. An example is the relationship between sunny weather and ice cream sales: they are correlated because both increase at the same time, but the sun doesn't cause people to buy ice cream, nor does buying ice cream cause the sun to shine. Instead, the warm weather (a third variable) causes both.  \n",
        "\n",
        "    Correlation : It is a statistical term which depicts the degree of association between two random variables. In data analysis it is often used to determine the amount to which they relate to one another. Three types of correlation-\n",
        "* Positive correlation - If with increase in random variable A, random variable B increases too, or vice versa.\n",
        "* Negative correlation - If increase in random variable A leads to a decrease in B, or vice versa.\n",
        "* No correlation - When both the variables are completely unrelated and change in one leads to no change in other.\n",
        "\n",
        "Causation : Causation between random variables A and B implies that A and B have a cause-and-effect relationship with one another. Or we can say existence of one gives birth to other, and we say A causes B or vice versa. Causation is also termed as causality.\n",
        "\n",
        "Correlation does not imply Causation.\n",
        "\n",
        "Correlation and Causation can exist at the same time also, so definitely correlation doesn't imply causation."
      ],
      "metadata": {
        "id": "_y29ZUIS7uSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "    An optimizer is an algorithm or function used to modify the attributes of a neural network, such as weights and learning rate, in order to reduce the loss function [2]. Essentially, optimizers help a model achieve the best possible performance by iteratively adjusting internal parameters to minimize the difference between its predictions and the actual data (the \"loss\").\n",
        "\n",
        "* SGD (Stochastic Gradient Descent)\n",
        "Stochastic Gradient Descent (SGD) updates the model parameters using the gradient of the loss function with respect to the weights. It is efficient, but can be slow, especially in complex models, due to noisy gradients and small updates.\n",
        "\n",
        "Syntax: tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(1)])\n",
        "\n",
        "Compile the model with SGD optimizer\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01,\n",
        "\n",
        "momentum=0.9), loss='mse')\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "* Adam (Adaptive Moment Estimation)\n",
        "Adam combines the advantages of two other extensions of SGD: AdaGrad and RMSProp.\n",
        "\n",
        "It computes adaptive learning rates for each parameter by considering both first and second moments of the gradients. Adam is one of the most popular optimizers due to its efficient handling of sparse gradients and non-stationary objectives.\n",
        "\n",
        "Implementing Adam in Tensorflow using tf.keras.optimizers():\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(1)])\n",
        "\n",
        "Compile the model with Adam optimizer\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "* RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "RMSprop is an adaptive learning rate method, that divides the learning rate by an exponentially decaying average of squared gradients. This optimizer is effective for handling non-stationary objectives and is often used for training RNNs.\n",
        "\n",
        "RMSprop can be implemented in TensorFlow using tf.keras.optimizers.RMSprop():\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(1)])\n",
        "\n",
        "Compile the model with RMSprop optimizer\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001), loss='mse')\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "pzz1UeUo8nyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "\n",
        "    sklearn.linear_model is a powerful module within the popular Scikit-learn Python library, providing various algorithms for linear regression and classification, where the target variable is predicted as a linear combination of features (e.g., \\(y=\\beta _{0}+\\beta _{1}x_{1}+\\dots +\\beta _{n}x_{n}\\)). It contains classes like LinearRegression, Ridge, Lasso, and ElasticNet for finding the best-fit line or hyperplane to model data, used for tasks like predicting house prices or classifying data points\n",
        "\n",
        "    Key Classes & Functions:\n",
        "* LinearRegression: Implements Ordinary Least Squares (OLS) to find coefficients that minimize the sum of squared errors.\n",
        "* Ridge: A regression method that uses L2 regularization (shrinkage) to prevent overfitting.\n",
        "* Lasso: Uses L1 regularization, which can shrink some coefficients to zero, performing feature selection.\n",
        "* ElasticNet: Combines L1 and L2 regularization (a mix of Lasso and Ridge).\n",
        "* SGDRegressor/SGDClassifier: Algorithms that use Stochastic Gradient Descent for efficient training, especially on large datasets.\n",
        "\n",
        "How it Works (General Steps):\n",
        "\n",
        "Import: from sklearn.linear_model import LinearRegression.\n",
        "\n",
        "Instantiate: model = LinearRegression().\n",
        "\n",
        "Train (Fit): model.fit(X, y) (where X are features, y are targets).\n",
        "\n",
        "Predict: model.predict(X_new)."
      ],
      "metadata": {
        "id": "KSo0gvPk-3RS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "    \n",
        "    The model.fit() method is a function used to train a machine learning model by iterating over the entire dataset a specified number of times (epochs). During this process, the model learns from the data by adjusting its internal parameters (weights and biases) to minimize a loss function using an optimization algorithm.\n",
        "\n",
        "    What model.fit() Does\n",
        "The training involves repeatedly processing data in batches within each epoch:\n",
        "* Forward Pass: Data is input to the model to make predictions.\n",
        "* Loss Calculation: The difference between predictions and actual values is calculated using a loss function.\n",
        "* Backward Pass: Gradients of the loss are calculated with respect to the model's parameters.\n",
        "* Weight Update: The optimizer adjusts parameters to reduce the loss.\n",
        "\n",
        "The method returns a History object containing training and optionally validation loss and metric values across epochs.\n",
        "\n",
        "Required Arguments\n",
        "\n",
        "For supervised learning, the essential arguments are the input data and corresponding targets:\n",
        "\n",
        "x (Input Data): The training data (e.g., NumPy array, tensor, or dataset).\n",
        "\n",
        "y (Target Data): The target labels or values corresponding to x (e.g., NumPy array, tensor). This is not needed if x is a dataset providing (inputs, targets).\n",
        "\n",
        "Common Optional Arguments\n",
        "\n",
        "Useful optional arguments include:\n",
        "batch_size: The number of samples processed per parameter update (defaults to 32).\n",
        "* epochs: The total number of iterations over the training dataset.\n",
        "* verbose: Controls the level of training output displayed (0, 1, or 2).\n",
        "* validation_split: A fraction of training data used for validation at the end of each epoch.\n",
        "* validation_data: A separate dataset for validation, overriding validation_split.\n",
        "* callbacks: A list of objects to perform actions during training, such as saving the model or early stopping.\n",
        "* shuffle: Whether to shuffle training data before each epoch (defaults to True)."
      ],
      "metadata": {
        "id": "8mpePlEE_dE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "    \n",
        "    The model.predict() function is used to generate predictions from a trained machine learning model based on new, unseen input data. It passes the input data through the model's architecture and returns the output values (e.g., class labels or continuous values), without performing any further training or evaluation.\n",
        "\n",
        "    What model.predict() Does\n",
        "* Generates Output: It uses the learned parameters of the trained model to process new inputs and produce an output.\n",
        "* For Inference Only: The primary purpose is inference (making predictions) rather than training or evaluation. It does not use true labels or compute performance metrics.\n",
        "* Returns Raw Predictions: The output depends on the model type. For regression, it returns continuous values. For classification, it typically returns the predicted class labels, or in some libraries like Keras, the raw output tensor (e.g., probabilities for each class), which might need post-processing to get the final labels.\n",
        "* Batch Processing: It is designed for efficient batch processing of large datasets, which can be faster than processing individual samples.\n",
        "\n",
        "Required Arguments\n",
        "\n",
        "The main argument that must be provided to model.predict() is the new input data for which you want predictions.\n",
        "* Input Data (X_new or similar): The function takes one primary argument, which is the data to be predicted upon.\n",
        "* Format: This data must be in a format compatible with the library and model type (e.g., a NumPy array, a pandas DataFrame, a tf.data.Dataset, etc.).\n",
        "* Shape: The data's structure (e.g., number of features, dimensions) must match the format the model was originally trained on. For many models, the input should be a 2D array where rows are samples and columns are features.\n",
        "\n",
        "Additional, optional arguments vary by specific library (e.g., Keras, scikit-learn, R) and can include:\n",
        "\n",
        "batch_size for controlling memory usage during prediction.\n",
        "\n",
        "verbose to control output logs.\n",
        "\n",
        "Other model-specific options like newdata in R."
      ],
      "metadata": {
        "id": "XZhywfMBAaC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What are continuous and categorical variables?\n",
        "\n",
        "   Continuous variables are numerical data that can take any value within a range (like height, temperature, time), representing measurements, while categorical variables are descriptive labels that divide data into distinct groups or categories (like hair color, gender, dog breed) and are non-numerical. Continuous variables are quantitative (measurements), whereas categorical variables are qualitative (descriptions).\n",
        "\n",
        "   Continuous Variables (Quantitative)\n",
        "* Definition: Variables that can be measured and can have infinite possible values between any two points on a scale, often involving decimals.\n",
        "* Examples: Height (e.g., 1.75m, 1.751m), weight, temperature, income, age.\n",
        "* Subtypes: Can be further divided into interval (e.g., temperature in Celsius) and ratio (e.g., height) variables.\n",
        "\n",
        "Categorical Variables (Qualitative)\n",
        "* Definition: Variables that represent groups or categories, describing qualities rather than quantities.\n",
        "* Examples: Hair color (blonde, brown), country of origin, type of car, survey responses like \"Yes/No\".\n",
        "* Subtypes: Can be nominal (no order, e.g., gender), ordinal (ordered, e.g., satisfaction levels), or dichotomous (two categories, e.g., true/false).\n",
        "\n"
      ],
      "metadata": {
        "id": "RrYt63SGBaVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "    Feature scaling is a data preprocessing step that transforms numerical features to a common scale, preventing features with larger ranges (like income) from overpowering those with smaller ranges (like age) in models, which improves performance, speeds up algorithm convergence (especially for gradient descent), and ensures fairness by giving all features equal importance. Methods include Normalization (Min-Max scaling to 0-1 range) and Standardization (Z-score scaling to mean=0, std=1).\n",
        "\n",
        "    How it helps in Machine Learning:\n",
        "* Prevents Feature Dominance: Without scaling, a feature like 'salary' (thousands) can unfairly influence a model more than 'age' (0-100) just because its numbers are bigger, leading to biased results.\n",
        "* Speeds Up Convergence: Algorithms using gradient descent (like neural networks, SVMs) converge much faster when features are scaled, as the cost function's contours become more spherical, allowing for more efficient steps.\n",
        "* Improves Model Performance: By giving features equal footing, models can learn more robust patterns, leading to better accuracy and generalization on unseen data.\n",
        "* Essential for Distance-Based Algorithms: Algorithms like K-Nearest Neighbors (KNN), K-Means, and Principal Component Analysis (PCA) rely on distance calculations, which are highly sensitive to feature scales.\n",
        "\n",
        "Common Scaling Techniques:\n",
        "* Min-Max Scaling (Normalization): Rescales data to a fixed range, usually [0]. Formula: (x - min(x)) / (max(x) - min(x)).\n",
        "* Standardization (Z-Score Scaling): Transforms data to have a mean of 0 and a standard deviation of 1. Formula: (x - mean(x)) / std_dev(x).\n",
        "* Robust Scaling: Uses median and interquartile range, making it less sensitive to outliers."
      ],
      "metadata": {
        "id": "6kgJAnRFDDam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "    \n",
        "    In Python, scaling (often called feature scaling) is primarily performed using the sklearn.preprocessing module from the Scikit-learn library. Different techniques are used depending on the data's distribution and the machine learning algorithm being used.\n",
        "\n",
        "    The two most common methods are Standardization (Z-score normalization) and Min-Max Scaling (Normalization).\n",
        "\n",
        "    1. Standardization (Z-score)-Standardization transforms the data to have a mean of 0 and a standard deviation of 1 (unit variance). This method is less affected by outliers than Min-Max Scaling and is suitable when data follows a normal distribution.\n",
        "    \n",
        "    Formula: \\(z=(x-u)/s\\) (where \\(u\\) is the mean and \\(s\\) is the standard deviation).\n",
        "    \n",
        "    Python Implementation (using StandardScaler):\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "Sample data\n",
        "\n",
        "data = {'feature1': [10, 20, 30, 40, 50], 'feature2': [1, 2, 3, 4, 100]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "Initialize the StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "Fit and transform the data\n",
        "\n",
        "It is important to fit the scaler on the training data and then transform both\n",
        "\n",
        "the training and test sets using the same scaler object to avoid data leakage.\n",
        "\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "Convert the scaled data back to a DataFrame for readability\n",
        "\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "\n",
        "print(scaled_df)\n",
        "\n",
        "Min-Max Scaling (Normalization)-Min-Max Scaling transforms features by scaling each value to a fixed range, usually between 0 and 1. This method is useful when the scale of a feature is irrelevant or when algorithms that don't assume a specific data distribution are used (e.g., neural networks). However, it is sensitive to outliers, which can skew the minimum and maximum values.\n",
        "\n",
        "Formula: \\(X_{scaled}=(X-X_{min})/(X_{max}-X_{min})\\).\n",
        "\n",
        "Python Implementation (using MinMaxScaler):\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "Sample data\n",
        "\n",
        "data = {'feature1': [10, 20, 30, 40, 50], 'feature2': [1, 2, 3, 4, 100]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "Initialize the MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "Fit and transform the data\n",
        "\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "Convert the scaled data back to a DataFrame\n",
        "\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
        "\n",
        "print(scaled_df)\n",
        "\n",
        "Other Scaling Methods\n",
        "\n",
        "Robust Scaling: Uses the median and interquartile range (IQR) for scaling, making it robust to outliers and skewed distributions.\n",
        "\n",
        "Normalization (Vector Norm): Scales each data sample (row) to have a unit norm (length of 1), useful for algorithms using cosine similarity (e.g., text classification).\n",
        "\n",
        "Key Considerations\n",
        "\n",
        "When to Scale: Feature scaling is a crucial part of data preprocessing for many machine learning algorithms, particularly those that use distance measures (like K-Means, K-Nearest Neighbors, and SVMs with RBF kernels) or gradient descent (like Linear/Logistic Regression).\n",
        "\n",
        "When Not to Scale: Tree-based models (Decision Trees, Random Forests) and Naive Bayes are typically not affected by feature scaling.\n",
        "\n",
        "Data Leakage: Always perform the fit operation on the training data only, then use the transform operation to scale both the training and testing datasets to prevent data leakage.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7OZiFpbB7vxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "    The sklearn.preprocessing package in the scikit-learn library provides a wide range of functions and transformer classes to prepare raw data for machine learning models. Preprocessing is a crucial step to ensure features are on a comparable scale, handle missing values, and convert non-numerical data into formats that estimators can use, which generally improves model performance and stability.\n",
        "\n",
        "    Key Functionality and Tools:\n",
        "\n",
        "The module offers various techniques to transform data:\n",
        "* Feature Scaling: Adjusting the magnitude of numerical features so that they are on a similar scale.\n",
        "\n",
        "* StandardScaler standardizes features by removing the mean and scaling to unit variance (Z-score normalization).\n",
        "* MinMaxScaler and MaxAbsScaler scale features to a specific range, often between [0, 1] or [-1, 1], respectively.\n",
        "* RobustScaler uses more robust estimates for the center and range of data, making it suitable when outliers are present.\n",
        "* Normalization: Scaling individual samples to have unit norm, which is useful for algorithms that use dot products or kernels to quantify similarity.\n",
        "* Normalizer performs this operation using L1 or L2 norms.\n",
        "\n",
        "* Encoding Categorical Features: Converting text or integer-based categorical features into a numerical format suitable for machine learning models.\n",
        "* OneHotEncoder transforms each category into a new binary feature column.\n",
        "* LabelEncoder converts labels (target variables) into numerical integers (0 to n-1 classes).\n",
        "* Imputation of Missing Values: Inferring and filling in missing data points, typically with the mean, median, or most frequent value of the column.\n",
        "SimpleImputer is the primary class for this purpose.\n",
        "* Data Transformation:\n",
        "* Binarizer applies a threshold to numerical features to get boolean values.\n",
        "* PolynomialFeatures generates new features by considering polynomial combinations of existing features, adding complexity to the model.\n",
        "* FunctionTransformer allows you to convert an arbitrary Python function into a transformer object that fits into a scikit-learn pipeline."
      ],
      "metadata": {
        "id": "HikQytZ19K4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "    \n",
        "    One of the most important steps in preparing data for training a ML model is splitting the dataset into training and testing sets. This simply means dividing the data into two parts: one to train the machine learning model (training set), and another to evaluate how well it performs on unseen data (testing set). The training set is used to fit the model, and the statistics of the training set are known. The second set is called the test data set which is solely used for predictions.\n",
        "\n",
        "We’ll see how to split a dataset into train and test sets using Python. We'll use scikit-learn library to perform the split efficiently. Whether you're working with numerical data, text, or images, this is an essential part of any supervised machine learning workflow.\n",
        "\n",
        "Installation:\n",
        "The scikit-learn library can be installed using pip:\n",
        "\n",
        "pip install scikit-learn\n",
        "\n",
        "Dataset Splitting\n",
        "Scikit-learn is one of the most widely used machine learning libraries in Python. It provides a range of tools for building models, pre-processing data, and evaluating performance. For splitting datasets, it provides a handy function called train_test_split() within the model_selection module, making it simple to divide your data into training and testing sets.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "* *arrays: The data you want to split. This can be in the form of lists, arrays, pandas DataFrames, or matrices.\n",
        "* test_size: A number between 0.0 and 1.0 that tells what portion of the data should go into the test set. For example, 0.2 means 20% of the data will be used for testing.\n",
        "* train_size: this is a number between 0.0 and 1.0 that tells what portion of the data should go into the training set. If not set, it’s automatically calculated based on the test_size.\n",
        "* random_state: A number that makes sure the split is the same every time you run the code. It’s like setting a seed for the shuffle.\n",
        "* shuffle: If True, the data is shuffled before splitting. This helps make the train and test sets more random. It’s True by default.\n",
        "* stratify: This helps keep the same class distribution in both the train and test sets. It’s useful especially for classification problems.\n",
        "\n",
        "In the example, we first import pandas and sklearn. Then, we load the CSV file using the read_csv() function. This stores the data in a DataFrame called df. we want to predict the house price, which is in the last column so we set that as y (target). All the other columns are used as features, stored in X.\n",
        "\n",
        "We use train_test_split() to split the data:\n",
        "\n",
        "test_size=0.05 means 5% of the data is used for testing, and 95% for training.\n",
        "random_state=0 ensures the split is the same every time we run the code."
      ],
      "metadata": {
        "id": "nYj6JDekCVUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv('Real-estate.csv')\n",
        "\n",
        "df.head()   # will display if last line or in separate cell\n",
        "\n",
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:, -1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.05, random_state=0\n",
        ")\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "pz7lcYUnFsrQ",
        "outputId": "022ab245-72a6-467f-f30e-76e87c1e9811"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad6d219a-fcff-4aac-b82b-5353ea4ad8f0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ad6d219a-fcff-4aac-b82b-5353ea4ad8f0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Real-estate.csv to Real-estate (9).csv\n",
            "(393, 7)\n",
            "(21, 7)\n",
            "(393,)\n",
            "(21,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "    \n",
        "    Data encoding is the process of converting information (text, images, sounds) into a specific format or code for efficient storage, transmission, or processing by computers, ensuring compatibility across systems, like turning letters into binary (0s and 1s) using schemes such as ASCII or Unicode, or preparing categorical data (like \"Red\", \"Green\") for machine learning models using techniques like Label Encoding. Essentially, it's a language translator for digital systems, changing raw data into a usable, structured form, with decoding being the reverse process.  \n",
        "\n",
        "    Key Functions & Examples:\n",
        "* For Transmission: Converts data into signals (like analog waves) for sending over networks, preventing corruption.\n",
        "For Storage: Saves data in standardized formats (e.g., UTF-8 for text) so different applications can read it.\n",
        "* For Machine Learning: Transforms raw data, especially text or categories, into numerical features that algorithms can understand, using methods like One-Hot Encoding or Label Encoding.\n",
        "* For Web URLs: Encodes special characters (like spaces, slashes) so URLs remain valid and secure.\n",
        "\n",
        "How it Works:\n",
        "* Input: Raw data (e.g., the letter 'A', a color name).\n",
        "* Encoding: A rule or algorithm converts it to a new format (e.g., 'A' becomes 01000001 in binary; \"Red\" becomes the number 1).\n",
        "Processing/Transmission: The encoded data is used.\n",
        "* Decoding: The reverse process converts the code back to the original information.\n",
        "\n",
        "Examples of Encoding Types:\n",
        "* Binary Encoding: The most fundamental, converting everything to 0s and 1s.\n",
        "* Unicode/ASCII: Standard sets for representing text characters.\n",
        "* One-Hot Encoding: Creates new binary columns for each category (e.g., 'Color_Red', 'Color_Green').\n",
        "* URL Encoding: Replaces unsafe characters with % followed by hex codes (e.g., space becomes %20)."
      ],
      "metadata": {
        "id": "NBZ1D1PoHBuE"
      }
    }
  ]
}